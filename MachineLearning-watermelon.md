---
title: Machine Learning watermelon 读书笔记
date: 2016-12-27
tags: [Machine Learning,读书笔记]
---

本文记录西瓜书的读书笔记。

<!--more-->
## 自助法 ##
- 有放回的多次抽样生成训练集，剩余的作为测试集。
- 测试集大小(1-1/m)^m，取极限得1/e，约36.8%。
- 自助法改变了数据集的分布，会引入估计偏差。


## 调参与最终模型 ##
- 训练数据分为训练集和验证集，验证集可用来调参
- 模型优化完后，需在全量数据集上重新模型，再交付出去


## 偏差与反差 ##
- Error = bias + variance + nosie
- noise是数据本身带来的，算法不可解，泛化误差的下界，刻画问题本身的难度

## 过采样&欠采样 ##
- 过采样：重复采样正样本，容易出现过拟合。可以考虑插值的方式产生额外的正样本
- 欠采样：随机丢弃负样本，会丢失一些重要信息。可以将负样本划分为多个集合，采用集成学习

## 决策树 划分选择 ##
- 信息增益(ID3)
- 信息增益偏向分支较多的特征，使用信息增益率。(信息增益率偏向分支较少的特征，C4.5:先选出信息增益高于平均值的特征，在从中选择增益率最高的)
- 基尼指数(CART)：sum(pi*(1-pi))

## pruning ##
- 标准:泛化性能是否提升，预留部分数据进行校验
- prepruning:节约训练时间；“贪心算法”陷入局部低点，二次展开可能有更优解，容易欠拟合
- postpruning:训完一棵树，往上减枝，训练时间开销大

## 缺失值 ##
- 选择划分特征时:只使用当前特征有值的样本
- 划分特征下缺失值样本：1.单独一棵子树 2.按取值的比例切分开

## Backpropagation ##
- 标准BP算法 vs 累计BP算法 类似于 SGD vs 标准GD
- 缓解过拟合:1.early stopping(边训练边预测，检测overfitting) 2.regularization

## "跳出"局部极小 ##
- 多个初始化参数。
- “模拟退火"：以一定概率接受"次优解"
- SGD：有一定随机性，局部极小时，梯度不一定为0
 `启发式，缺失理论保障` `有可能“跳出”全局最小`

## 深度学习 ##
- 训练效率低：当前计算能力大幅提高
- 易陷入过拟合：当前训练数据的大幅增加
- Wide vs Deep：从增加模型复杂度来说，增加隐层比增加隐层内神经元个数，因为增加了激活函数嵌套的层数，所以更有效。
- 隐层过多，使用标准BP算法，出现误差发散(diverge)，难以收敛
- pre-training + fine-tuning. 无监督逐层"预训练" + 有监督整体"微调"

## 特征选择 ##
- 过滤式：训练前过滤，按相关统计量进行过滤(书上例子：Relief。cross entropy应该也行)
- 包裹式：直接拿特征子集训练，选择最优的。"量身定制"，多次训练，开销大
- 嵌入式：模型训练和特征选择同时进行。L1正则化就是一种	嵌入式特征选择方法。决策树应该也算(无效特征不会用上)

## 稀疏表示/字典学习 ##
- 将稠密矩阵分解为字典向量和稀疏矩阵
- 交替优化求解